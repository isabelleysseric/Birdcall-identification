{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "WBMCm6ZCfEsF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bab48a0-44dc-4601-ec2e-3dee8b3f3626"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "from shutil import copyfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "\n",
        "from IPython.display import display, Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import librosa\n",
        "import librosa as lb\n",
        "import soundfile as sf\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize,transforms, ToTensor\n",
        "\n",
        "!pip install git+https://github.com/ulaval-damas/glo4030-labs.git\n",
        "from deeplib.history import History\n",
        "from deeplib.datasets import train_valid_loaders\n",
        "import torchvision.models as models\n",
        "from deeplib.visualization import plot_images\n",
        "\n",
        "from poutyne import Experiment\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "\n",
        "# For use TPU\n",
        "#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# dev = xm.xla_device()\n",
        "#DEVICE = torch.device(dev if torch.xla_device(dev).is_available() else \"cpu\")  #\"cuda\" \n",
        "\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install GPUtil\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "gpu_usage()\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from numba import cuda\n",
        "cuda.select_device(0)\n",
        "cuda.close()\n",
        "cuda.select_device(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ulaval-damas/glo4030-labs.git\n",
            "  Cloning https://github.com/ulaval-damas/glo4030-labs.git to /tmp/pip-req-build-2c9gu4zo\n",
            "  Running command git clone -q https://github.com/ulaval-damas/glo4030-labs.git /tmp/pip-req-build-2c9gu4zo\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (0.9.1+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (1.1.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (4.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (0.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (5.5.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (1.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from deeplib==0.1) (7.1.2)\n",
            "Collecting poutyne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/d1/a38eba0c8bd9d5a1694dd8de1aa40f03cec7e1efb2d6e74373fae3f6b619/Poutyne-1.4-py3-none-any.whl (134kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->deeplib==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deeplib==0.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->deeplib==0.1) (2018.9)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->deeplib==0.1) (5.0.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->deeplib==0.1) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->deeplib==0.1) (5.3.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplib==0.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplib==0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplib==0.1) (0.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplib==0.1) (1.0.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplib==0.1) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplib==0.1) (56.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->deeplib==0.1) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplib==0.1) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->deeplib==0.1) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->deeplib==0.1) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->deeplib==0.1) (4.8.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->deeplib==0.1) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->deeplib==0.1) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.1.0->ipykernel->deeplib==0.1) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->deeplib==0.1) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->deeplib==0.1) (22.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplib==0.1) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->deeplib==0.1) (0.7.0)\n",
            "Building wheels for collected packages: deeplib\n",
            "  Building wheel for deeplib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplib: filename=deeplib-0.1-cp37-none-any.whl size=10554 sha256=a433d4edf8e7a7d3c2afa598b12bd832b404a9bd64eac45ee5c36656fa40098b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vkozlk34/wheels/5e/d1/5f/50a2caa0ee64b5f4a2ed0d59696c01fe8f4f467123ff81095f\n",
            "Successfully built deeplib\n",
            "Installing collected packages: poutyne, deeplib\n",
            "Successfully installed deeplib-0.1 poutyne-1.4\n",
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=271a9a16392ab7c34c753c38be1fff9d03822f5d79e417654405dfba9e3831e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  0% |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weakproxy at 0x7f6a4dfd0d10 to Device at 0x7f6a565787d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFobjNsI8KTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0b9241-0de6-456e-900f-b49a8c9f1b52"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR3wrSf2Yt0I"
      },
      "source": [
        "# FONCTION UTILITAIRES\n",
        "\n",
        "Tirée du notebook : https://www.kaggle.com/kneroma/clean-fast-simple-bird-identifier-inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkMcNv7BH1fl"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=3, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "# Fonctions pour transférer un audio en image\n",
        "def get_spectrograms(filepath,savepath,k):\n",
        "    \n",
        "    # Open the file with librosa (limited to the first 15 seconds)\n",
        "    sig, rate = librosa.load(filepath, sr=SAMPLE_RATE, offset=None, duration=15)\n",
        "    \n",
        "    # Split signal into five second chunks\n",
        "    sig_splits = []\n",
        "    for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):\n",
        "        split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]\n",
        "\n",
        "        # End of signal?\n",
        "        if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):\n",
        "            break\n",
        "        \n",
        "        sig_splits.append(split)\n",
        "        \n",
        "    # Extract mel spectrograms for each audio chunk\n",
        "    j = 0\n",
        "    saved_samples = []\n",
        "    for chunk in sig_splits:\n",
        "        \n",
        "        # Spectogramm\n",
        "        hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))\n",
        "        mel_spec = librosa.feature.melspectrogram(y=chunk, \n",
        "                                                  sr=SAMPLE_RATE, \n",
        "                                                  n_fft=1024, \n",
        "                                                  hop_length=hop_length, \n",
        "                                                  n_mels=SPEC_SHAPE[0], \n",
        "                                                  fmin=FMIN, \n",
        "                                                  fmax=FMAX)\n",
        "    \n",
        "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n",
        "        \n",
        "        # Amplitude\n",
        "        # BINS_PER_OCTAVE = 12 * 3\n",
        "        # N_OCTAVES = 7\n",
        "        # mel_amp = librosa.amplitude_to_db(np.abs(librosa.cqt(y=sig, sr=rate,\n",
        "        #                                 bins_per_octave=BINS_PER_OCTAVE,\n",
        "        #                                 n_bins=N_OCTAVES * BINS_PER_OCTAVE)),\n",
        "        #                                 ref=np.max)\n",
        "        \n",
        "        # display\n",
        "        # fig, ax = plt.subplots()\n",
        "        # librosa.display.specshow(mel_amp, y_axis='cqt_hz', sr=rate,\n",
        "        #                         bins_per_octave=BINS_PER_OCTAVE,\n",
        "        #                         x_axis='time', ax=ax)\n",
        "\n",
        "        # Normalize\n",
        "        mel_spec -= mel_spec.min()\n",
        "        if mel_spec.max()!= 0:\n",
        "            mel_spec /= mel_spec.max()\n",
        "        else:\n",
        "            mel_spec /= 0.00001\n",
        "        \n",
        "        # Save as image file\n",
        "#         save_dir = os.path.join(output_dir, primary_label)\n",
        "#         if not os.path.exists(save_dir):\n",
        "#             os.makedirs(save_dir)\n",
        "#         save_path = os.path.join(save_dir, filepath.rsplit(os.sep, 1)[-1].rsplit('.', 1)[0] + \n",
        "#                                  '_' + str(s_cnt) + '.png')\n",
        "\n",
        "        im = Image.fromarray(mel_spec * 255.0).convert(\"1\")\n",
        "        save_path = savepath +'/'+ str(k)+'_'+ str(j) +'.jpg'\n",
        "        im.save(save_path)\n",
        "        imsave('result_col.png', im)\n",
        "        \n",
        "#         saved_samples.append(save_path)\n",
        "        j += 1\n",
        "        \n",
        "        \n",
        "    return im\n",
        "\n",
        "\n",
        "def make_dir(file_path):\n",
        "    os.makedirs(file_path)\n",
        "\n",
        "\n",
        "\n",
        "def separate_train_test(dataset_path, train_path, test_path):\n",
        "    \"\"\"\n",
        "    Cette fonction sépare les images de CUB200 en un jeu d'entraînement et de test.\n",
        "\n",
        "    dataset_path: Path où se trouve les images de CUB200\n",
        "    train_path: path où sauvegarder le jeu d'entraînement\n",
        "    test_path: path où sauvegarder le jeu de test\n",
        "    \"\"\"\n",
        "    class_index = 1\n",
        "    for classname in sorted(os.listdir(dataset_path)):\n",
        "        print(classname)\n",
        "        make_dir(os.path.join(train_path, classname))\n",
        "        make_dir(os.path.join(test_path, classname))\n",
        "        i = 0\n",
        "        \n",
        "        nb_file = len(os.listdir(os.path.join(dataset_path, classname)))\n",
        "        nb_test = ceil(nb_file*0.2)\n",
        "       \n",
        "        for file in sorted(os.listdir(os.path.join(dataset_path, classname))):\n",
        "            \n",
        "            file_path = os.path.join(dataset_path, classname, file)\n",
        "            \n",
        "            if i < nb_test:\n",
        "                get_spectrograms(file_path,os.path.join(test_path, classname),i)\n",
        "                \n",
        "            else:\n",
        "                get_spectrograms(file_path,os.path.join(train_path, classname),i)\n",
        "                \n",
        "            i += 1\n",
        "\n",
        "#         class_index += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYOTepiUZGDE"
      },
      "source": [
        "# VARIABLES GLOBALES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqjxY9PhZKfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb89c0a-cd27-429f-c87b-298d21e297a7"
      },
      "source": [
        "nb_classes = 397\n",
        "\n",
        "# spectogram\n",
        "SAMPLE_RATE = 32_000\n",
        "SIGNAL_LENGTH= 5\n",
        "SPEC_SHAPE = (48, 128)\n",
        "FMIN = 500\n",
        "FMAX = 12500\n",
        "\n",
        "transform = transforms.Compose(\n",
        "        [transforms.Resize([448, 448]),\n",
        "         transforms.ToTensor()]) \n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# test_audio_path = Path(\"C:/Users/isabelle/Desktop/DL/FINAL/birdclef-2021/test_soundscapes\")\n",
        "# train_audio_path = Path(\"C:/Users/isabelle/Desktop/DL/FINAL/birdclef-2021/train_soundscapes\")\n",
        "# train_short_audio_path = (\"C:/Users/isabelle/Desktop/DL/FINAL/birdclef-2021/train_short_audio\")\n",
        "# label_path = Path(\"C:/Users/isabelle/Desktop/DL/FINAL/birdclef-2021/train_soundscape_labels.csv\")\n",
        "\n",
        "train_path = \"./kaggle/working/train_img\"\n",
        "test_path  = \"./kaggle/working/test_img\"\n",
        "dataset_path = \"./MyDrive/data/train_short_audio/\"\n",
        "\n",
        "# separate_train_test(dataset_path,train_path,test_path)\n",
        "# !zip -r train.zip \"/content/drive/MyDrive/Birds/train (2)\"\n",
        "# !zip -r test.zip \"/content/drive/MyDrive/Birds/test (1)\"\n",
        "\n",
        "!unzip -qq \"/content/drive/MyDrive/Birds/train (2)\"\n",
        "!unzip -qq \"/content/drive/MyDrive/Birds/test (1)\"\n",
        "print('Done')\n",
        "\n",
        "train_data = ImageFolder('./kaggle/working/train_img',transform=transform)\n",
        "test_data = ImageFolder('./kaggle/working/test_img',transform=transform)\n",
        "print(\"ImageFolder 1...\")\n",
        "\n",
        "\n",
        "# Images Noir & Blanc\n",
        "print(\"Images Noir & Blanc\")\n",
        "from pathlib import Path\n",
        "from typing import Callable\n",
        "from tqdm import tqdm\n",
        "\n",
        "base_path_train = Path(\"./kaggle/working/train_img\")\n",
        "base_path_test = Path(\"./kaggle/working/test_img\")\n",
        "\n",
        "def grey_to_black(origin_image_path, new_image_path):\n",
        "    im_gray = cv2.imread(origin_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    (thresh, im_bw) = cv2.threshold(im_gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    im_bw = cv2.threshold(im_gray, thresh, 255, cv2.THRESH_BINARY)[1]\n",
        "    cv2.imwrite(new_image_path, im_bw)\n",
        "\n",
        "def edit_images(base_path: Path, folder_to_switch: str, new_folder_name: str, filter_fct: Callable):\n",
        "    for file in tqdm(base_path.glob('**/*')):\n",
        "        if file.is_file():\n",
        "            new_path = Path(str(file).replace(folder_to_switch, new_folder_name))\n",
        "            if not new_path.parent.exists():\n",
        "                new_path.parent.mkdir(parents=True)\n",
        "            filter_fct(str(file), str(new_path))\n",
        "\n",
        "edit_images(base_path=base_path_train, folder_to_switch='train_img', new_folder_name='train_img_2', filter_fct=grey_to_black)\n",
        "edit_images( base_path=base_path_test, folder_to_switch='test_img', new_folder_name='test_img_2', filter_fct=grey_to_black)\n",
        "\n",
        "train_data = ImageFolder('./kaggle/working/train_img_2/',transform=transform)\n",
        "test_data = ImageFolder('./kaggle/working/test_img_2/',transform=transform)\n",
        "print(\"ImageFolder 2...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "536it [00:00, 5341.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ImageFolder 1...\n",
            "Images Noir & Blanc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "137724it [00:53, 2569.73it/s]\n",
            "35285it [00:13, 2609.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ImageFolder 2...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNbk4XPWfEsN"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bXJB3rOrfEsP"
      },
      "source": [
        "\n",
        "\n",
        "def train(model, dataset, n_epoch, batch_size, learning_rate, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Entraîne un réseau de neurones de classification pour un certain nombre d'epochs \n",
        "    avec PyTorch.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): Un réseau de neurones instancié avec PyTorch.\n",
        "        dataset (Dataset): Un jeu de données PyTorch.\n",
        "        n_epoch (int): Le nombre d'epochs.\n",
        "        batch_size (int): La taille des batchs.\n",
        "        learning_rate (float): Le taux d'apprentissage pour SGD.\n",
        "        use_gpu (bool): Si les données doivent être envoyées sur GPU.\n",
        "    \n",
        "    Returns:\n",
        "        Retourne un objet History permettant de faire des graphiques\n",
        "        de l'évolution de l'entraînement.\n",
        "    \"\"\"\n",
        "    # La classe History vient de deeplib. Elle va nous permettre de faire les graphiques\n",
        "    # donnant l'évolution de la perte et de l'exactitude (accuracy).\n",
        "    history = History()\n",
        "    \n",
        "    # La fonction de perte que nous utilisons ici est l'entropy croisée\n",
        "    # L'optimiseur que nous utilisons ici est le classique SGD.\n",
        "    # Des liens vers la documentation de PyTorch sont en commentaires.\n",
        "    criterion = nn.CrossEntropyLoss()  # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
        "    scheduler = ReduceLROnPlateau(optimizer =optimizer, patience = 4, verbose = True)\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "    \n",
        "    # La fonction train_valid_loaders vient de deeplib. Elle nous retourne deux DataLoaders:\n",
        "    # un pour l'ensemble d'entraînement et un pour l'ensemble de test. Essentiellement,\n",
        "    # DataLoader est une classe de PyTorch nous permettant de faire des batchs avec la taille\n",
        "    # désirée. La fonction train_valid_loaders effectue la répartition aléatoire des exemples\n",
        "    # en entraînement et en validation.\n",
        "    train_loader, valid_loader = train_valid_loaders(dataset, batch_size=batch_size)\n",
        "\n",
        "    # C'est ici que la boucle d'entraînement commence. On va donc faire n_epochs epochs.\n",
        "    for i in range(n_epoch):\n",
        "        torch.cuda.empty_cache()\n",
        "        # Les réseaux de neurones avec PyTorch ont un méthode train() en plus d'une méthode \n",
        "        # eval(). Ces deux méthodes indiquent au réseau s'il est en entraînement ou bien en test.\n",
        "        # Ceci permet au réseau de modifier son comportement en fonction. On va le voir plus tard\n",
        "        # certaines couches agissent différemment selon le mode, nommément le dropout et la \n",
        "        # batch normalization.\n",
        "        model.train()\n",
        "        \n",
        "        # La prochaine ligne active simplement le calcul du gradient. Le gradient est ce qui va\n",
        "        # nous permettre de mettre à jour les poids du réseau de neurones. En test, le calcul du\n",
        "        # gradient sera désactivé étant qu'il n'est pas nécessaire et qu'il peut engendrer des \n",
        "        # fuites de mémoire si la rétro-propagation n'est pas effectuée.\n",
        "        with torch.enable_grad():\n",
        "            # À chaque epoch, on parcourt l'ensemble d'entraînement au complet via le DataLoader\n",
        "            # qui nous le retourne en batch (x, y) comme mentionné plus haut. La variable inputs \n",
        "            # correspond donc à une batch d'exemples (x) et targets correspond à une batch \n",
        "            # d'étiquettes (y).\n",
        "            for inputs, targets in train_loader:\n",
        "                # On envoie les exemples et leurs étiquettes sur GPU via la méthode cuda() si \n",
        "                # demandé.\n",
        "                if use_gpu:\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "\n",
        "                # La méthode zero_grad() de l'optimiseur permet de mettre la valeur du gradient\n",
        "                # à zéro de façon à effacer le gradient calculé auparavant. Si ceci n'était pas \n",
        "                # fait, le nouveau gradient serait additionné à l'ancien gradient ce qui poserait\n",
        "                # problème.\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # C'est ici que finalement le réseau de neurones est appelé. On lui donne en entrée\n",
        "                # un exemple et en sortie il nous donne ses prédictions (ici, des scores de \n",
        "                # classification).\n",
        "                output = model(inputs)\n",
        "\n",
        "                # Une fois nos prédictions obtenues, on calcule la perte avec la fonction de perte \n",
        "                # qui nous retourne un tenseur scalaire. \n",
        "                loss = criterion(output, targets)\n",
        "                # Ce tenseur scalaire nous permet de calculer le gradient. C'est ce que la méthode\n",
        "                # backward() vient faire pour nous via la rétropropagation.\n",
        "                loss.backward()\n",
        "                # Une fois que le gradient est calculé, il ne reste que mettre à jour les poids du\n",
        "                # réseau de neurones. C'est ce que la méthode step() de l'optimiseur vient faire\n",
        "                # pour nous.\n",
        "                optimizer.step()\n",
        "                #del loss, output\n",
        "\n",
        "        # Après chaque epoch d'entraînement, on va venir calculer la perte et l'exactitude \n",
        "        # (accuracy) sur l'ensemble d'entraînement et de validation.\n",
        "        train_acc, train_loss = validate(model, train_loader, use_gpu)\n",
        "        val_acc, val_loss = validate(model, valid_loader, use_gpu)\n",
        "        early_stopping(val_loss, model)\n",
        "        \n",
        "        history.save(dict(acc=train_acc, val_acc=val_acc, loss=train_loss, val_loss=val_loss, lr=learning_rate))\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        print(f'Epoch {i} - Train acc: {train_acc:.2f} - Val acc: {val_acc:.2f} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f}')\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "    return history, model\n",
        "\n",
        "\n",
        "def validate(model, valid_loader, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Test un réseau de neurones de classification pour un certain nombre d'epochs \n",
        "    avec PyTorch.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): Un réseau de neurones instancié avec PyTorch.\n",
        "        valid_loader (DataLoader): Un DataLoader PyTorch tel qu'instancié dans train() \n",
        "            et test().\n",
        "        use_gpu (bool): Si les données doivent être envoyées sur GPU.\n",
        "            \n",
        "    Returns:\n",
        "        Retourne un tuple (exactitude, perte) pour les données du DataLoader en argument.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Les étapes de la fonction validate est très similaire à celle de la fonction train.\n",
        "    # Essentiellement, le réseau est mis en mode évaluation au lieu d'entraînement et le \n",
        "    # calcul du gradient est désactivé. Il n'y a bien sûr pas d'utilisation d'un optimiseur.\n",
        "    true = []\n",
        "    pred = []\n",
        "    val_loss = []\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in valid_loader:\n",
        "            if use_gpu:\n",
        "                inputs = inputs.cuda()\n",
        "                targets = targets.cuda()\n",
        "\n",
        "            output = model(inputs)\n",
        "\n",
        "            predictions = output.max(dim=1)[1]\n",
        "\n",
        "            val_loss.append(criterion(output, targets).item())\n",
        "            true += targets.cpu().numpy().tolist()\n",
        "            pred += predictions.cpu().numpy().tolist()\n",
        "\n",
        "    return accuracy_score(true, pred) * 100, sum(val_loss) / len(val_loss)\n",
        "\n",
        "\n",
        "def test(model, dataset, batch_size, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Test un réseau de neurones de classification pour un certain nombre d'epochs \n",
        "    avec PyTorch. La fonction affiche l'exactitude et la perte moyenne.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): Un réseau de neurones instancié avec PyTorch.\n",
        "        dataset (Dataset): Un jeu de données PyTorch.\n",
        "        batch_size (int): La taille des batchs.\n",
        "        use_gpu (bool): Si les données doivent être envoyées sur GPU.\n",
        "    \"\"\"\n",
        "    test_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    test_acc, test_loss = validate(model, test_loader, use_gpu=use_gpu)\n",
        "    print('Test acc: {:.2f} - Test loss: {:.4f}'.format(test_acc, test_loss))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "navo3HYpagvu"
      },
      "source": [
        "# PROGRAMME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KlHFdv_4fEsR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "dfa73bfd-75d0-4bad-ab1d-dafed66b361f"
      },
      "source": [
        "# Architecture du reseau\n",
        "# resnet18 = models.resnet18(pretrained = True)\n",
        "# ct = 0\n",
        "# for child in resnet18.children():\n",
        "#     print(ct)\n",
        "#     print(child)\n",
        "\n",
        "#     ct+=1\n",
        "\n",
        "# Model ResNet\n",
        "#nb_classes = 397\n",
        "#model = models.resnet34(pretrained=False)\n",
        "# ct = 0\n",
        "# for child in resnet18.children():\n",
        "#     if ct <= 4:\n",
        "#         for param in child.parameters():\n",
        "#             param.requires_grad = False\n",
        "#     ct+=1\n",
        "#model.fc = nn.Linear(model.fc.in_features, nb_classes)\n",
        "\n",
        "# Model VGG\n",
        "# model = models.vgg16(pretrained=False)\n",
        "# num_ftrs = model.classifier[0].in_features\n",
        "# model.classifier[0].out_features = nn.Linear(num_ftrs, nb_classes)\n",
        "\n",
        "!pip install GPUtil\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "gpu_usage()\n",
        "\n",
        "# !pip install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp37-cp37m-linux_x86_64.whl\n",
        "# import torch\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# from numba import cuda\n",
        "# cuda.select_device(0)\n",
        "# cuda.close()\n",
        "# cuda.select_device(0)\n",
        "\n",
        "#gpu_usage()\n",
        "\n",
        "# def free_gpu_cache():\n",
        "#     print('Initial GPU Usage')\n",
        "#     gpu_usage()\n",
        "\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     cuda.select_device(0)\n",
        "#     cuda.close()\n",
        "#     cuda.select_device(0)\n",
        "\n",
        "#     print('GPU Usage after emptying the cache')\n",
        "\n",
        "#     gpu_usage()\n",
        "\n",
        "#free_gpu_cache()\n",
        "\n",
        "\n",
        "#import logging\n",
        "#torch.cuda.empty_cache()\n",
        "#torch.cuda.memory_summary()\n",
        "\n",
        "# import gc\n",
        "# del model, \n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "#try:\n",
        "nb_classes = 397\n",
        "epoch = 20\n",
        "batch_size = 256\n",
        "learning_rate = 0.01\n",
        "\n",
        "model = models.resnet34(pretrained=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, nb_classes)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "history, model = train(model, train_data, epoch, batch_size, learning_rate, use_gpu=True)\n",
        "history.display()\n",
        "test(model, test_data, batch_size, use_gpu=True)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "#except BaseException as err:\n",
        "    #logging.exception(err)\n",
        "    #torch.cuda.empty_cache()\n",
        "\n",
        "#finally:\n",
        "    #torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  5% |\n",
            "Collecting torch==1.1.0\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.1.0-cp37-cp37m-linux_x86_64.whl (770.7MB)\n",
            "\u001b[K     |████████████████████████████████| 770.7MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.1.0) (1.19.5)\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed torch-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-63e483fd7f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp37-cp37m-linux_x86_64.whl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid argument"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q3-xLVbGfEsR"
      },
      "source": [
        "# history.(dict(acc=train_acc, val_acc=val_acc, loss=train_loss, val_loss=val_loss, lr=learning_rate))\n",
        "# score = test(model, test_data, batch_size, use_gpu=True)\n",
        "# top_k_accuracy_score(y_true=history.acc, y_score=score, 10, normalize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DIqBZqt_fEsR"
      },
      "source": [
        "# nb_classes = 397\n",
        "# resnet50= models.resnet50(pretrained = False)\n",
        "\n",
        "# resnet50.fc = nn.Linear(resnet50.fc.in_features, nb_classes)\n",
        "# epoch = 30\n",
        "# batch_size = 16\n",
        "# learning_rate = 0.001\n",
        "# if torch.cuda.is_available():\n",
        "#   resnet50.cuda()\n",
        "#history,resnet50 = train(resnet50, train_data, epoch, batch_size, learning_rate, use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPPNjaPJg1ty"
      },
      "source": [
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm import tqdm\n",
        "# from tqdm import tqdm\n",
        "# import warnings\n",
        "\n",
        "# warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "\n",
        "# # Just Seaborn Barplot w/ Common Input Format\n",
        "# def bar_plot(x, y, title='Training Soundscape : 5 second segment identification', xlim=None, ylim=None,\n",
        "#              xticklabels=None, yticklabels=None, xlabel=None, ylabel=None,\n",
        "#              figsize=(10, 4), axis_grid='x', xrotation=None, yrotation=None):\n",
        "\n",
        "#     cmap = sns.color_palette(\"mako\")\n",
        "#     fig, ax = plt.subplots(figsize=figsize)\n",
        "#     plt.title(title)\n",
        "\n",
        "#     for i in ['top', 'right', 'bottom', 'left']:\n",
        "#         ax.spines[i].set_color('black')\n",
        "\n",
        "#     ax.spines['top'].set_visible(True);\n",
        "#     ax.spines['right'].set_visible(False)\n",
        "#     ax.spines['bottom'].set_visible(False);\n",
        "#     ax.spines['left'].set_visible(False)\n",
        "\n",
        "#     sns.barplot(x=x, y=y, edgecolor='black', ax=ax, palette=cmap)\n",
        "#     ax.set_xlim(xlim);\n",
        "#     ax.set_ylim(ylim)\n",
        "#     #     ax.set_xticklabels(xticklabels);ax.set_yticklabels(yticklabels)\n",
        "#     plt.xlabel(xlabel);\n",
        "#     plt.ylabel(ylabel)\n",
        "#     ax.grid(axis=axis_grid, ls='--', alpha=0.3)\n",
        "#     plt.xticks(rotation=xrotation)\n",
        "#     plt.yticks(rotation=yrotation)\n",
        "#     plt.savefig('bar_out.png');\n",
        "#     plt.show();\n",
        "\n",
        "# # Visualise the Birds present in Recordings\n",
        "# bar_plot(x=x[1:], y=y[1:], figsize=(20, 5), xrotation=90, title='Soundscape Data: Birds Labeled in All Recordings')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}